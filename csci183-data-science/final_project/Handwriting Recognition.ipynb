{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting Recognition Using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the Problem\n",
    "\n",
    "Identifying handwritten characters is a common problem in computer vision applications. For example, the USPS uses algorithms to recognize handwritten addresses on envelopes to enable automated mail sorting. Solving this problem using traditional strategies (i.e. edge detection) can be challenging. Can we use data science techniques to recognize handwritten digits (e.g. 0, 1, 2, ... , 9) with a reasonable degree of accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire the Data\n",
    "The MNIST data set is a collection of 70,000 labeled images of handwritten digits. http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "We use a pre-processed and pickled version of the dataset made by deeplearning.net. The pickled version can be downloaded from: http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import linear_model, cross_validation, metrics, preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for convinience, we use a pickled version of the MNIST dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse, Mine, and Refine the Data\n",
    "\n",
    "'train_set' is a tuple. The first element is an array where each row represents an image. An image consists of 28x28 pixels where each pixel is represented as a float.\n",
    "\n",
    "The second element is an array of the corresponding labels for each image.\n",
    "\n",
    "We can extract a single training example and plot it using pyplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets\\example_image.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# PLOT IMAGE\n",
    "# image is rc/binary format\n",
    "plt.rc('image', cmap='binary')\n",
    "# reshape first training image into 28x28 format\n",
    "plt.matshow((train_set[0][0]).reshape(28, 28))\n",
    "# label of first training image\n",
    "plt.title(\"Label: %d\" % train_set[1][0])\n",
    "plt.show()\n",
    "\n",
    "print(repr(train_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 70,000 images are split as follows:\n",
    "* 50,000 are put into the training set\n",
    "* 10,000 are put into the validation set\n",
    "* 10,000 are put into the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 50000\n",
      "Length of validation set: 10000\n",
      "Length of test set: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of training set: %d\" % len(train_set[0]))\n",
    "print(\"Length of validation set: %d\" % len(valid_set[0]))\n",
    "print(\"Length of test set: %d\" % len(test_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model\n",
    "\n",
    "Since this is a classification task, we use train a logistic regression model to classify images into one of 10 categories: 0, 1, 2, ..., 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(verbose = 1)\n",
    "model.fit(train_set[0][0:20000], train_set[1][0:20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9114\n",
      "[(7, 7), (2, 2), (1, 1), (0, 0), (4, 4), (1, 1), (4, 4), (9, 9), (6, 5), (9, 9), (0, 0), (6, 6), (9, 9), (0, 0), (1, 1), (5, 5), (9, 9), (7, 7), (5, 3), (4, 4), (9, 9), (6, 6), (6, 6), (5, 5), (4, 4), (0, 0), (7, 7), (4, 4), (0, 0), (1, 1), (3, 3), (1, 1), (3, 3), (4, 4), (7, 7), (2, 2), (7, 7), (1, 1), (3, 2), (1, 1), (1, 1), (7, 7), (4, 4), (2, 2), (3, 3), (5, 5), (1, 1), (2, 2), (4, 4), (4, 4), (6, 6), (3, 3), (5, 5), (5, 5), (6, 6), (0, 0), (4, 4), (1, 1), (9, 9), (5, 5), (7, 7), (2, 8), (9, 9), (2, 3), (7, 7), (4, 4), (7, 6), (4, 4), (3, 3), (0, 0), (7, 7), (0, 0), (2, 2), (9, 9), (1, 1), (7, 7), (3, 3), (1, 2), (9, 9), (7, 7), (7, 7), (6, 6), (2, 2), (7, 7), (8, 8), (4, 4), (7, 7), (3, 3), (6, 6), (1, 1), (3, 3), (6, 6), (9, 9), (3, 3), (1, 1), (4, 4), (1, 1), (7, 7), (6, 6), (9, 9)]\n",
      "(6, 5)\n",
      "(5, 3)\n",
      "(3, 2)\n",
      "(2, 8)\n",
      "(2, 3)\n",
      "(7, 6)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_set[0])\n",
    "print metrics.accuracy_score(test_set[1], pred)\n",
    "print zip(pred[0:100], test_set[1][0:100])\n",
    "for tuple in zip(pred[0:100], test_set[1][0:100]):\n",
    "    if tuple[0] != tuple[1]:\n",
    "        print tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization\n",
    "Let's normalize the data to see if that improves performance! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\preprocessing\\data.py:184: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "train_X = train_set[0][0:20000]\n",
    "\n",
    "train_y = train_set[1][0:20000]\n",
    "X_scaled = preprocessing.scale(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scaled = linear_model.LogisticRegression(verbose = 1)\n",
    "model_scaled.fit(X_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9013\n",
      "(6, 5)\n",
      "(8, 3)\n",
      "(3, 2)\n",
      "(2, 8)\n",
      "(1, 6)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = model_scaled.predict(preprocessing.scale(test_set[0]))\n",
    "print metrics.accuracy_score(test_set[1], pred)\n",
    "for tuple in zip(pred[0:100], test_set[1][0:100]):\n",
    "    if tuple[0] != tuple[1]:\n",
    "        print tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy went down by 1% after we scaled the data. One possible explanation for this is that our dataset has small numbers that are sparse (i.e. the number of 0s outnumber the number of non-zero values). This means that the standard deviation is very close to 0 and that can cause numeric issues when scaling the data. In this case, it is probably better to use the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Present the Results\n",
    "\n",
    "Using logistic regression, we correctly recognize images in the test set ~92% of the time. This is pretty accurate but our model is probably not good enough to be used in critical services like the USPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also graph the learned coefficients for each digit to get an visualization of how our logistic regression model has learned the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display coefficients for each digit\n",
    "for val, rep in enumerate(model.coef_):\n",
    "    plt.rc('image', cmap='binary')\n",
    "    # reshape first training image into 28x28 format\n",
    "    plt.matshow((rep.reshape(28, 28)))\n",
    "    # label of first training image\n",
    "    plt.title(val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets\\1.png'>\n",
    "<img src='assets\\2.png'>\n",
    "<img src='assets\\3.png'>\n",
    "<img src='assets\\4.png'>\n",
    "<img src='assets\\5.png'>\n",
    "<img src='assets\\6.png'>\n",
    "<img src='assets\\7.png'>\n",
    "<img src='assets\\8.png'>\n",
    "<img src='assets\\9.png'>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
